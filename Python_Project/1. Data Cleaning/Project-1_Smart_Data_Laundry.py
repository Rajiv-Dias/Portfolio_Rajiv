# -*- coding: utf-8 -*-
"""Project 1 - Smart Data Laundry

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Z9bld7xrIAY8bQ0xhPjua0sLY2SqZsg

# 1. Description

ğŸš€ Smart Data Laundry: Automated Cleaning Pipeline
------------------------------------------------
Fungsi utama: Mengubah dataset mentah menjadi data siap pakai.

Workflow:
1. Ingesting   : Memuat file CSV/Excel dari path lokal.
2. Deduplicate : Identifikasi & isolasi duplikat (Copy preserved).
3. Sanitizing  : Pembersihan missing values:
                 - Numerik -> Imputasi Mean ($ \mu $)
                 - Objek/Lainnya -> Drop Row
4. Exporting   : Output 'clean_data.csv' & Summary Report.

Technical Specs:
- Library: Pandas, NumPy, OS, Time, Random, Openpyxl, Xlrd
- Input Support: .csv, .xlsx, .xls
- Automation: Type-based duplicates and null handling
------------------------------------------------

# 2. Smart Data Laundry ğŸ§ºğŸ§ºğŸ§º
"""

# Import semua package yg diperlukan
import pandas as pd
import numpy as np
import time
import openpyxl
import xlrd
import os
import random

# 1. Main Step -- ğŸ’€ğŸ’€ğŸ’€
# 2. Bridging -- ğŸŒŠğŸŒŠ

# 1. Creating a Function with Path and Name as parameters ğŸ’€ğŸ’€ğŸ’€
def data_cleaning_master(data_path, data_name):
    print("thank you for giving us your data")

    # Bridging ğŸŒŠğŸŒŠ
    sec = random.randint(1, 3)
    print(f"\nChecking file... Estimated time: {sec} seconds")
    time.sleep(sec)


    # 2. Checking if the path/ dataset exists: ğŸ’€ğŸ’€ğŸ’€

    # A. If not exists
    if not os.path.exists(data_path):
        print('File not found. Please check the file path and try again.âŒ')
        return None # Return None agar program utama tahu ada error

    # B. If exists (Loading Data)
    if data_path.endswith('.csv'):
        print("Your dataset is csv")
        try:
            data = pd.read_csv(data_path, encoding_errors='ignore')
        except pd.errors.ParserError:
            print("ParserError detected. Attempting to read with semicolon delimiter.")
            try:
                data = pd.read_csv(data_path, encoding_errors='ignore', sep=';')
            except pd.errors.ParserError as e:
                print(f"Failed to read with semicolon delimiter either. Original error: {e}")
                print("Please check your file's delimiter. Common delimiters are ',', ';', or '\t'.")
                return None

    elif data_path.endswith('.xlsx'):
        print("Your dataset is Excel")
        data = pd.read_excel(data_path)

    else:
        print("Unknown dataset format")
        return None

    # 3. Changed all column names to lowercase for consistency
    data.columns = [col.lower() for col in data.columns]
    print("\nColumn names have been converted to lowercase for consistency.")


    # 4. Check dataset dimension ğŸ’€ğŸ’€ğŸ’€

    # Bridging ğŸŒŠğŸŒŠ
    num = random.randint(1, 4)
    print(f"\nChecking for dataset dimension... Estimated time: {num} seconds")
    time.sleep(num)
    print(f"Dataset contain:\nTotal rows: {data.shape[0]}\nTotal Columns {data.shape[1]}")



    # 5. Check number of duplicates and remove all the duplicates  ğŸ’€ğŸ’€ğŸ’€

    # Bridging ğŸŒŠğŸŒŠ
    sec = random.randint(1, 3)
    print(f"\nChecking for duplicates... Estimated time: {sec} seconds.")
    time.sleep(sec)

    duplicates = data.duplicated()
    total_duplicates = data.duplicated().sum()
    percentage = round((total_duplicates / data.shape[0]) * 100, 2)
    print(f"Total duplicate records found: {total_duplicates} \n{percentage} % of the total data.")


    # 6. Saving the duplicates in csv ğŸ’€ğŸ’€ğŸ’€
    if total_duplicates > 0:
        print(f"\nSaving the duplicates... Estimated time: {sec} seconds.")
        time.sleep(sec)
        duplicate_record = data[duplicates]
        duplicate_record.to_csv(f'{data_name}_duplicates.csv', index=None)
        # deleting duplicates
        data = data.drop_duplicates()


    # 7. Finding Missing Value ğŸ’€ğŸ’€ğŸ’€

    # Bridging ğŸŒŠğŸŒŠ
    sec = random.randint(1, 3)
    print(f"\nChecking for missing values... Estimated time: {sec} seconds.")
    time.sleep(sec)

    total_missing_value = data.isnull().sum().sum()
    missing_value_columns = data.isnull().sum()
    print(f"The dataset contains a total of {total_missing_value} missing values.")
    print(f"\nTotal Dataset missing values by column: \n{missing_value_columns}")


    # 8. Dealing with Missing Value ğŸ’€ğŸ’€ğŸ’€

    # Bridging ğŸŒŠğŸŒŠ
    sec = random.randint(2, 4)
    print(f"\nDealing with missing values... Estimated time: {sec} seconds.")
    time.sleep(sec)

    columns = data.columns
    for col in columns:
        if data[col].dtype in (float, int):
            data[col] = data[col].fillna(data[col].mean())
        else:
            data.dropna(subset=[col], inplace=True)

    # 9. Reset Index
    # Create a new sorted index after  dropping Duplicates and NA data.
    # Bridging ğŸŒŠğŸŒŠ
    sec = random.randint(2, 4)
    print(f"\nReset index... Estimated time: {sec} seconds.")
    time.sleep(sec)

    data = data.reset_index(drop=True)
    print("Index has been reset to ensure sequential numbering.")

    print(f"\nCongrats! Dataset is cleaned! \nNumber of Rows: {data.shape[0]}, Number of Columns {data.shape[1]}")


    # 10. Saving the cleaned dataset ğŸ’€ğŸ’€ğŸ’€
    # a. CSV
    data.to_csv(f"{data_name}_Clean_data.csv", index=None)
    print("\nDataset has been successfully saved as a CSV file. âœ…")


    # b. Excel
    data.to_excel(f"{data_name}_Clean_data.xlsx", index=None)
    print("Dataset has been successfully saved as an Excel file. âœ…")

    return data # Mengembalikan data yang sudah bersih



# 11. Testing and verifying the code.
if __name__ == '__main__':
    print('Welcome to Data Cleaning Master')
    # ask path and file name
    data_path = input('Please enter your dataset path: ')
    data_name = input('Please enter your dataset name: ')

    # Panggil fungsi dan simpan hasilnya ke variabel agar bisa dicek
    cleaned_df = data_cleaning_master(data_path, data_name)

    if cleaned_df is not None:
        print("\nAll processes completed successfully!")