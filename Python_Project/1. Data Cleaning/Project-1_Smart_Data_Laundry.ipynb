{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Description"
      ],
      "metadata": {
        "id": "0YV-52tHY5Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ðŸš€ Smart Data Laundry: Automated Cleaning Pipeline\n",
        "------------------------------------------------\n",
        "Fungsi utama: Mengubah dataset mentah menjadi data siap pakai.\n",
        "\n",
        "Workflow:\n",
        "1. Ingesting   : Memuat file CSV/Excel dari path lokal.\n",
        "2. Deduplicate : Identifikasi & isolasi duplikat (Copy preserved).\n",
        "3. Sanitizing  : Pembersihan missing values:\n",
        "                 - Numerik -> Imputasi Mean ($ \\mu $)\n",
        "                 - Objek/Lainnya -> Drop Row\n",
        "4. Exporting   : Output 'clean_data.csv' & Summary Report.\n",
        "\n",
        "Technical Specs:\n",
        "- Library: Pandas, NumPy, OS, Time, Random, Openpyxl, Xlrd\n",
        "- Input Support: .csv, .xlsx, .xls\n",
        "- Automation: Type-based duplicates and null handling\n",
        "------------------------------------------------\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eM_6WCe8XGZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Smart Data Laundry ðŸ§ºðŸ§ºðŸ§º"
      ],
      "metadata": {
        "id": "whLkTMpEoXwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import semua package yg diperlukan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import openpyxl\n",
        "import xlrd\n",
        "import os\n",
        "import random\n",
        "\n",
        "# 1. Main Step -- ðŸ’€ðŸ’€ðŸ’€\n",
        "# 2. Bridging -- ðŸŒŠðŸŒŠ\n",
        "\n",
        "# 1. Creating a Function with Path and Name as parameters ðŸ’€ðŸ’€ðŸ’€\n",
        "def data_cleaning_master(data_path, data_name):\n",
        "    print(\"thank you for giving us your data\")\n",
        "\n",
        "    # Bridging ðŸŒŠðŸŒŠ\n",
        "    sec = random.randint(1, 3)\n",
        "    print(f\"\\nChecking file... Estimated time: {sec} seconds\")\n",
        "    time.sleep(sec)\n",
        "\n",
        "\n",
        "    # 2. Checking if the path/ dataset exists: ðŸ’€ðŸ’€ðŸ’€\n",
        "\n",
        "    # A. If not exists\n",
        "    if not os.path.exists(data_path):\n",
        "        print('File not found. Please check the file path and try again.âŒ')\n",
        "        return None # Return None agar program utama tahu ada error\n",
        "\n",
        "    # B. If exists (Loading Data)\n",
        "    if data_path.endswith('.csv'):\n",
        "        print(\"Your dataset is csv\")\n",
        "        try:\n",
        "            data = pd.read_csv(data_path, encoding_errors='ignore')\n",
        "        except pd.errors.ParserError:\n",
        "            print(\"ParserError detected. Attempting to read with semicolon delimiter.\")\n",
        "            try:\n",
        "                data = pd.read_csv(data_path, encoding_errors='ignore', sep=';')\n",
        "            except pd.errors.ParserError as e:\n",
        "                print(f\"Failed to read with semicolon delimiter either. Original error: {e}\")\n",
        "                print(\"Please check your file's delimiter. Common delimiters are ',', ';', or '\\t'.\")\n",
        "                return None\n",
        "\n",
        "    elif data_path.endswith('.xlsx'):\n",
        "        print(\"Your dataset is Excel\")\n",
        "        data = pd.read_excel(data_path)\n",
        "\n",
        "    else:\n",
        "        print(\"Unknown dataset format\")\n",
        "        return None\n",
        "\n",
        "    # 3. Changed all column names to lowercase for consistency\n",
        "    data.columns = [col.lower() for col in data.columns]\n",
        "    print(\"\\nColumn names have been converted to lowercase for consistency.\")\n",
        "\n",
        "\n",
        "    # 4. Check dataset dimension ðŸ’€ðŸ’€ðŸ’€\n",
        "\n",
        "    # Bridging ðŸŒŠðŸŒŠ\n",
        "    num = random.randint(1, 4)\n",
        "    print(f\"\\nChecking for dataset dimension... Estimated time: {num} seconds\")\n",
        "    time.sleep(num)\n",
        "    print(f\"Dataset contain:\\nTotal rows: {data.shape[0]}\\nTotal Columns {data.shape[1]}\")\n",
        "\n",
        "\n",
        "\n",
        "    # 5. Check number of duplicates and remove all the duplicates  ðŸ’€ðŸ’€ðŸ’€\n",
        "\n",
        "    # Bridging ðŸŒŠðŸŒŠ\n",
        "    sec = random.randint(1, 3)\n",
        "    print(f\"\\nChecking for duplicates... Estimated time: {sec} seconds.\")\n",
        "    time.sleep(sec)\n",
        "\n",
        "    duplicates = data.duplicated()\n",
        "    total_duplicates = data.duplicated().sum()\n",
        "    percentage = round((total_duplicates / data.shape[0]) * 100, 2)\n",
        "    print(f\"Total duplicate records found: {total_duplicates} \\n{percentage} % of the total data.\")\n",
        "\n",
        "\n",
        "    # 6. Saving the duplicates in csv ðŸ’€ðŸ’€ðŸ’€\n",
        "    if total_duplicates > 0:\n",
        "        print(f\"\\nSaving the duplicates... Estimated time: {sec} seconds.\")\n",
        "        time.sleep(sec)\n",
        "        duplicate_record = data[duplicates]\n",
        "        duplicate_record.to_csv(f'{data_name}_duplicates.csv', index=None)\n",
        "        # deleting duplicates\n",
        "        data = data.drop_duplicates()\n",
        "\n",
        "\n",
        "    # 7. Finding Missing Value ðŸ’€ðŸ’€ðŸ’€\n",
        "\n",
        "    # Bridging ðŸŒŠðŸŒŠ\n",
        "    sec = random.randint(1, 3)\n",
        "    print(f\"\\nChecking for missing values... Estimated time: {sec} seconds.\")\n",
        "    time.sleep(sec)\n",
        "\n",
        "    total_missing_value = data.isnull().sum().sum()\n",
        "    missing_value_columns = data.isnull().sum()\n",
        "    print(f\"The dataset contains a total of {total_missing_value} missing values.\")\n",
        "    print(f\"\\nTotal Dataset missing values by column: \\n{missing_value_columns}\")\n",
        "\n",
        "\n",
        "    # 8. Dealing with Missing Value ðŸ’€ðŸ’€ðŸ’€\n",
        "\n",
        "    # Bridging ðŸŒŠðŸŒŠ\n",
        "    sec = random.randint(2, 4)\n",
        "    print(f\"\\nDealing with missing values... Estimated time: {sec} seconds.\")\n",
        "    time.sleep(sec)\n",
        "\n",
        "    columns = data.columns\n",
        "    for col in columns:\n",
        "        if data[col].dtype in (float, int):\n",
        "            data[col] = data[col].fillna(data[col].mean())\n",
        "        else:\n",
        "            data.dropna(subset=[col], inplace=True)\n",
        "\n",
        "    # 9. Reset Index\n",
        "    # Create a new sorted index after  dropping Duplicates and NA data.\n",
        "    # Bridging ðŸŒŠðŸŒŠ\n",
        "    sec = random.randint(2, 4)\n",
        "    print(f\"\\nReset index... Estimated time: {sec} seconds.\")\n",
        "    time.sleep(sec)\n",
        "\n",
        "    data = data.reset_index(drop=True)\n",
        "    print(\"Index has been reset to ensure sequential numbering.\")\n",
        "\n",
        "    print(f\"\\nCongrats! Dataset is cleaned! \\nNumber of Rows: {data.shape[0]}, Number of Columns {data.shape[1]}\")\n",
        "\n",
        "\n",
        "    # 10. Saving the cleaned dataset ðŸ’€ðŸ’€ðŸ’€\n",
        "    # a. CSV\n",
        "    data.to_csv(f\"{data_name}_Clean_data.csv\", index=None)\n",
        "    print(\"\\nDataset has been successfully saved as a CSV file. âœ…\")\n",
        "\n",
        "\n",
        "    # b. Excel\n",
        "    data.to_excel(f\"{data_name}_Clean_data.xlsx\", index=None)\n",
        "    print(\"Dataset has been successfully saved as an Excel file. âœ…\")\n",
        "\n",
        "    return data # Mengembalikan data yang sudah bersih\n",
        "\n",
        "\n",
        "\n",
        "# 11. Testing and verifying the code.\n",
        "if __name__ == '__main__':\n",
        "    print('Welcome to Data Cleaning Master')\n",
        "    # ask path and file name\n",
        "    data_path = input('Please enter your dataset path: ')\n",
        "    data_name = input('Please enter your dataset name: ')\n",
        "\n",
        "    # Panggil fungsi dan simpan hasilnya ke variabel agar bisa dicek\n",
        "    cleaned_df = data_cleaning_master(data_path, data_name)\n",
        "\n",
        "    if cleaned_df is not None:\n",
        "        print(\"\\nAll processes completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmzhPhh0rZeO",
        "outputId": "ffd060b1-4944-4df4-96c0-f19adbc47d63"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Data Cleaning Master\n",
            "Please enter your dataset path: 1. CovidDeaths_Raw_Data.csv\n",
            "Please enter your dataset name: 1. CovidDeath\n",
            "thank you for giving us your data\n",
            "\n",
            "Checking file... Estimated time: 3 seconds\n",
            "Your dataset is csv\n",
            "\n",
            "Column names have been converted to lowercase for consistency.\n",
            "\n",
            "Checking for dataset dimension... Estimated time: 1 seconds\n",
            "Dataset contain:\n",
            "Total rows: 308013\n",
            "Total Columns 16\n",
            "\n",
            "Checking for duplicates... Estimated time: 3 seconds.\n",
            "Total duplicate records found: 0 \n",
            "0.0 % of the total data.\n",
            "\n",
            "Checking for missing values... Estimated time: 2 seconds.\n",
            "The dataset contains a total of 1442018 missing values.\n",
            "\n",
            "Total Dataset missing values by column: \n",
            "iso_code                         0\n",
            "continent                    14649\n",
            "location                         0\n",
            "date                             0\n",
            "population                       0\n",
            "total_cases                  36108\n",
            "new_cases                     8815\n",
            "total_deaths                 56668\n",
            "new_deaths                    8717\n",
            "total_deaths_per_million     56668\n",
            "new_deaths_per_million        8717\n",
            "reproduction_rate           123196\n",
            "icu_patients                272184\n",
            "hosp_patients               271411\n",
            "weekly_icu_admissions       298631\n",
            "weekly_hosp_admissions      286254\n",
            "dtype: int64\n",
            "\n",
            "Dealing with missing values... Estimated time: 2 seconds.\n",
            "\n",
            "Reset index... Estimated time: 2 seconds.\n",
            "Index has been reset to ensure sequential numbering.\n",
            "\n",
            "Congrats! Dataset is cleaned! \n",
            "Number of Rows: 293364, Number of Columns 16\n",
            "\n",
            "Dataset has been successfully saved as a CSV file. âœ…\n",
            "Dataset has been successfully saved as an Excel file. âœ…\n",
            "\n",
            "All processes completed successfully!\n"
          ]
        }
      ]
    }
  ]
}